<!DOCTYPE html>
<html>

<head lang="en">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>DiVA360: Dynamic Visual-Audio Dataset for Immersive Neural Fields</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <!-- <link rel="icon" type="image/png" href=""> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="./DiVA/bootstrap.min.css">
    <link rel="stylesheet" href="./DiVA/font-awesome.min.css">
    <link rel="stylesheet" href="./DiVA/codemirror.min.css">
    <link rel="stylesheet" href="./DiVA/app.css">

    <link rel="stylesheet" href="./DiVA/bootstrap.min(1).css">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-E0ZMW34H4P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-E0ZMW34H4P');
    </script>

    <script src="./DiVA/jquery.min.js"></script>
    <script src="./DiVA/bootstrap.min.js"></script>
    <script src="./DiVA/codemirror.min.js"></script>
    <script src="./DiVA/clipboard.min.js"></script>

    <script src="./DiVA/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                DiVA360: Dynamic Visual-Audio Dataset for Immersive Neural Fields<br>
                <small>
                    <!--                     NeurIPS 2022 -->
                </small>
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        Anonymous Authors
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>

                <p class="text-justify">
                    Recent advance in neural radiance field (NeRF) has achieved high-fidelity photorealistic synthesis
                    for images and dynamic videos from novel views, paving the way for immersive experiences in visual
                    content. Nevertheless, the representation of motion and sound, which are integral components of
                    real-world multimedia experiences, often falls short in current NeRF research, which primarily
                    focuses on static scenes or brief dynamic sequences lasting only a few seconds. The gap between
                    existing research and practical application stems largely from the absence of extensive
                    long-sequence multiview datasets and benchmarks. To address this, we present the first large-scale
                    omniview visual-audio dataset of real-world dynamic scenes to facilitate research in tackling
                    various NeRF challenges. Our dataset captures dynamic human activities and objects in motion,
                    reflecting the complexity and diversity of daily life. Alongside this rich dynamic content, our
                    dataset also incorporates canonicalized static objects and is comprehensively annotated with
                    captions for every scene. We present our system design to capture the audio and 360$^{\circ}$ view
                    images from 53 synchronized high-resolution cameras at 120 fps.</p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a>.
                <p></p>
            </div>
        </div>

</body>

</html>